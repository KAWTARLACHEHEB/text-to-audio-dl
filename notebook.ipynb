# 1. Install Required Libraries
!pip install torch torchaudio matplotlib pandas numpy librosa soundfile
!pip install transformers datasets

from google.colab import drive
drive.mount('/content/drive')

# Text-to-Speech Notebook
# ===============================



# 2. Import Libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchaudio
import librosa
import librosa.display
import soundfile as sf
from torch.utils.data import Dataset, DataLoader
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
from datasets import load_dataset
from IPython.display import Audio, display

# 3. Load and Explore Dataset
# Define paths to data
wavs_dir = "/content/drive/MyDrive/speech_data/textToaudio app/wavs"  # Directory containing .wav files
metadata_file = "/content/drive/MyDrive/speech_data/textToaudio app/metadata.csv"  # File containing transcripts

# Load metadata
df = pd.read_csv(metadata_file, sep="|", header=None)
if len(df.columns) == 3:
    df.columns = ["file_name", "transcript", "normalized_transcript"]
else:
    df.columns = ["file_name", "transcript"]
    df["normalized_transcript"] = df["transcript"]

print(f"Total samples: {len(df)}")
print("Sample data:")
display(df.head())

# Display audio information for one sample
sample_file = os.path.join(wavs_dir, df.iloc[0]["file_name"] + ".wav")

# Add a check for the first sample file before loading
if not os.path.exists(sample_file):
    print(f"Warning: First sample file not found at {sample_file}. Skipping audio info display.")
else:
    waveform, sample_rate = torchaudio.load(sample_file)
    print(f"Sample rate: {sample_rate}")
    print(f"Waveform shape: {waveform.shape}")

    # Plot waveform
    plt.figure(figsize=(10, 3))
    plt.plot(waveform[0].numpy())
    plt.title("Waveform")
    plt.xlabel("Sample")
    plt.ylabel("Amplitude")
    plt.show()

    # Play a sample audio
    display(Audio(sample_file, autoplay=False))


# 4. Create a Dataset Class
class TTSDataset(Dataset):
    def __init__(self, metadata_df, wavs_dir):
        self.metadata_df = metadata_df
        self.wavs_dir = wavs_dir

    def __len__(self):
        return len(self.metadata_df)

    def __getitem__(self, idx):
        row = self.metadata_df.iloc[idx]
        file_name = row["file_name"]
        transcript = row["normalized_transcript"]

        # Load audio file
        wav_path = os.path.join(self.wavs_dir, file_name + ".wav")

        # The check for existence is now done before creating the dataset,
        # so we can load directly here.
        waveform, sample_rate = torchaudio.load(wav_path)

        # Convert to mono if needed
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        return {
            "file_name": file_name,
            "waveform": waveform,
            "sample_rate": sample_rate,
            "transcript": transcript
        }

# Custom collate function to pad waveforms (already defined later in the notebook, moved up for clarity)
def custom_collate_fn(batch):
    # Find the maximum length in the batch
    max_len = max(item["waveform"].shape[1] for item in batch)

    # Pad waveforms to the maximum length
    padded_waveforms = []
    for item in batch:
        waveform = item["waveform"]
        padding = max_len - waveform.shape[1]
        # Pad on the right (end of the audio) with zeros
        padded_waveform = torch.nn.functional.pad(waveform, (0, padding))
        padded_waveforms.append(padded_waveform)

    # Stack the padded waveforms
    waveforms_batch = torch.stack(padded_waveforms, dim=0)

    # Collate other elements
    file_names = [item["file_name"] for item in batch]
    sample_rates = [item["sample_rate"] for item in batch]
    transcripts = [item["transcript"] for item in batch]

    return {
        "file_name": file_names,
        "waveform": waveforms_batch,
        "sample_rate": sample_rates,
        "transcript": transcripts
    }

# Filter the DataFrame to include only existing files
print("Checking for existing audio files...")
df['wav_path'] = df['file_name'].apply(lambda x: os.path.join(wavs_dir, x + ".wav"))
initial_rows = len(df)
df = df[df['wav_path'].apply(os.path.exists)].reset_index(drop=True)
print(f"Removed {initial_rows - len(df)} entries with missing audio files.")
print(f"Remaining samples after filtering: {len(df)}")

# Create dataset and dataloader
dataset = TTSDataset(df, wavs_dir)
# Pass the custom collate function to the DataLoader
dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)

# Test the dataloader
# This test should now pass if there are remaining samples in the filtered dataframe
try:
    batch = next(iter(dataloader))
    print(f"Batch size: {len(batch['file_name'])}")
    print(f"File names: {batch['file_name']}")
    print(f"Sample transcripts: {batch['transcript'][0]}")
except StopIteration:
    print("DataLoader is empty after filtering. No valid audio files found.")


# 5. Implement Text-to-Speech with Pretrained Models
# Load pre-trained models
processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

# Get speaker embeddings (using a default speaker from a dataset)
from datasets import load_dataset

# Try with explicit cache directory
embeddings_dataset = load_dataset(
    "Matthijs/cmu-arctic-xvectors",
    split="validation",
    cache_dir=None,  # Don't use cache
    download_mode="force_redownload"
)
speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)


# Function to synthesize speech
def synthesize_speech(text, output_file=None):
    inputs = processor(text=text, return_tensors="pt")

    # Generate speech
    speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)

    # Save the audio file if requested
    if output_file:
        sf.write(output_file, speech.numpy(), samplerate=16000)

    return speech.numpy(), 16000

# 6. Generate and Compare Speech
# Get a sample transcript from our dataset
# Ensure dataset is not empty before attempting to get a sample
if len(df) > 0:
    sample_idx = 0  # Use the first available sample after filtering
    sample_text = df.iloc[sample_idx]["transcript"]
    print(f"Sample text: {sample_text}")

    # Generate speech
    output_file = "generated_speech.wav"
    generated_audio, gen_sample_rate = synthesize_speech(sample_text, output_file)

    # Display original and generated audio
    original_file = os.path.join(wavs_dir, df.iloc[sample_idx]["file_name"] + ".wav")
    print("Original Audio:")
    display(Audio(original_file, autoplay=False))

    print("Generated Audio:")
    display(Audio(generated_audio, rate=gen_sample_rate, autoplay=False))
else:
    print("Cannot perform Section 6, 7, 8, 9 as no valid audio files were found.")


# 7. Build a Speech Generation Interface
# Ensure dataset is not empty before defining/using function that relies on it
if len(df) > 0:
    def generate_speech_from_text(text_input):
        """Generate speech from text input and display audio player"""
        audio, sample_rate = synthesize_speech(text_input)
        print(f"Generated speech for: {text_input}")
        display(Audio(audio, rate=sample_rate, autoplay=False))
        return audio, sample_rate

    # Test with custom text
    custom_text = "This is a test of the text to speech system. How does it sound to you?"
    generate_speech_from_text(custom_text)


# 8. Batch Generation Function
# Ensure dataset is not empty before defining/using function that relies on it
if len(df) > 0:
    def generate_multiple_samples(texts, output_dir="generated_samples"):
        """Generate speech for multiple text samples"""
        os.makedirs(output_dir, exist_ok=True)

        for i, text in enumerate(texts):
            output_file = os.path.join(output_dir, f"sample_{i:03d}.wav")
            audio, _ = synthesize_speech(text, output_file)
            print(f"Generated sample {i}: {text[:50]}...")

        print(f"All samples generated in {output_dir} directory")

    # Generate a few samples from our dataset (use up to the number of available samples)
    sample_texts = df["transcript"].iloc[:min(3, len(df))].tolist()
    generate_multiple_samples(sample_texts)


# 9. Visualize Spectrograms
# Ensure dataset is not empty before defining/using function that relies on it
if len(df) > 0:
    def plot_spectrogram(audio, sample_rate, title="Spectrogram"):
        """Plot spectrogram of audio signal"""
        if isinstance(audio, torch.Tensor):
            audio = audio.numpy()

        if audio.ndim > 1:
            audio = audio[0]  # Take first channel if multi-channel

        plt.figure(figsize=(10, 4))
        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)
        librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sample_rate)
        plt.colorbar(format='%+2.0f dB')
        plt.title(title)
        plt.tight_layout()
        plt.show()

    # Get original and generated audio for comparison
    # Use the original_file path from Section 6, ensuring it exists
    original_file_for_spectrogram = os.path.join(wavs_dir, df.iloc[sample_idx]["file_name"] + ".wav")
    if os.path.exists(original_file_for_spectrogram):
        original_waveform, original_sr = torchaudio.load(original_file_for_spectrogram)
        original_audio = original_waveform[0].numpy()

        # Plot spectrograms
        plot_spectrogram(original_audio, original_sr, "Original Audio Spectrogram")
        plot_spectrogram(generated_audio, gen_sample_rate, "Generated Audio Spectrogram")
    else:
        print(f"Original file not found for spectrogram display: {original_file_for_spectrogram}")


# 10. Create a Simple TTS Function for User Input
def text_to_speech_converter():
    """Simple interactive TTS function"""
    text = input("Enter text to convert to speech: ")
    if text.strip():
        audio, sr = synthesize_speech(text, "user_generated.wav")
        display(Audio(audio, rate=sr, autoplay=False))
        print("Audio saved to 'user_generated.wav'")
    else:
        print("No text entered")

# Run the interactive function
# text_to_speech_converter() # Keep commented out as before

# 11. Save the Model and Configuration (Optional)
output_dir = "saved_tts_model"
os.makedirs(output_dir, exist_ok=True)

# Save the models for later use
processor.save_pretrained(output_dir)
model.save_pretrained(output_dir)
vocoder.save_pretrained(os.path.join(output_dir, "vocoder"))

print(f"Models saved to {output_dir}")

# 12. Create a Demo Application
from IPython.display import clear_output

def tts_demo():
    """Interactive TTS demo application"""
    print("Text-to-Speech Demo")
    print("-------------------")
    print("Enter text below or type 'exit' to quit")

    while True:
        text = input("\nText to convert: ")
        if text.lower() == 'exit':
            print("Exiting demo")
            break

        if text.strip():
            clear_output(wait=True)
            print("Text-to-Speech Demo")
            print("-------------------")
            print(f"Converting: {text}")
            audio, sr = synthesize_speech(text, f"demo_output.wav")
            display(Audio(audio, rate=sr, autoplay=True))
            print("\nEnter new text or type 'exit' to quit")
        else:
            print("No text entered")

# Run the demo
# tts_demo()  # Uncomment to run the interactive demo



# Define speech synthesis function
def synthesize_speech(text, output_file=None):
    inputs = processor(text=text, return_tensors="pt")

    # Generate speech
    speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)

    # Save the audio file if requested
    if output_file:
        sf.write(output_file, speech.numpy(), samplerate=16000)

    return speech.numpy(), 16000

# Test with custom text
custom_text = input("Enter text to synthesize speech: ")
output_file = "generated_speech.wav"

print(f"Generating speech for: {custom_text}")
audio, sr = synthesize_speech(custom_text, output_file)

# Play generated audio
print("Generated Audio:")
display(Audio(audio, rate=sr, autoplay=False))

# Download the generated file



# Import the 'files' module for downloading in Google Colab
from google.colab import files
import zipfile
import os

# Directory containing the saved models
output_dir = "saved_tts_model"
zip_filename = f"{output_dir}.zip"

# Create the zip file
print(f"Creating zip file: {zip_filename}")
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    # Walk through the directory and add all files to the zip
    for root, dirs, files_to_zip in os.walk(output_dir):
        for file in files_to_zip:
            file_path = os.path.join(root, file)
            # Arcname is the path relative to the zip file's root
            arcname = os.path.relpath(file_path, output_dir)
            zipf.write(file_path, arcname)

print("Zip file created successfully.")

# Download the zip file containing the saved models
try:
    files.download(zip_filename)
    print(f"Downloaded {zip_filename}")
except FileNotFoundError:
    print(f"Error: Zip file not found after creation: {zip_filename}")
except Exception as e:
    print(f"An error occurred during download: {e}")


